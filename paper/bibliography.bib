@misc{livathinos2025doclingefficientopensourcetoolkit,
  title         = {Docling: An Efficient Open-Source Toolkit for AI-driven Document Conversion},
  author        = {Nikolaos Livathinos and Christoph Auer and Maksym Lysak and Ahmed Nassar and Michele Dolfi and Panos Vagenas and Cesar Berrospi Ramis and Matteo Omenetti and Kasper Dinkla and Yusik Kim and Shubham Gupta and Rafael Teixeira de Lima and Valery Weber and Lucas Morin and Ingmar Meijer and Viktor Kuropiatnyk and Peter W. J. Staar},
  year          = {2025},
  eprint        = {2501.17887},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL},
  url           = {https://arxiv.org/abs/2501.17887}
}

@misc{markitdown,
  author       = {Microsoft},
  title        = {markitdown},
  year         = {2024},
  howpublished = {\url{https://github.com/microsoft/markitdown}},
  note         = {GitHub repository}
}

@misc{grattafiori2024llama3herdmodels,
  title         = {The Llama 3 Herd of Models},
  author        = {Aaron Grattafiori and Abhimanyu Dubey and Abhinav Jauhri and Abhinav Pandey and Abhishek Kadian and Ahmad Al-Dahle and Aiesha Letman and Akhil Mathur and Alan Schelten and Alex Vaughan and Amy Yang and Angela Fan and Anirudh Goyal and Anthony Hartshorn and Aobo Yang and Archi Mitra and Archie Sravankumar and Artem Korenev and Arthur Hinsvark and Arun Rao and Aston Zhang and Aurelien Rodriguez and Austen Gregerson and Ava Spataru and Baptiste Roziere and Bethany Biron and Binh Tang and Bobbie Chern and Charlotte Caucheteux and Chaya Nayak and Chloe Bi and Chris Marra and Chris McConnell and Christian Keller and Christophe Touret and Chunyang Wu and Corinne Wong and Cristian Canton Ferrer and Cyrus Nikolaidis and Damien Allonsius and Daniel Song and Danielle Pintz and Danny Livshits and Danny Wyatt and David Esiobu and Dhruv Choudhary and Dhruv Mahajan and Diego Garcia-Olano and Diego Perino and Dieuwke Hupkes and Egor Lakomkin and Ehab AlBadawy and Elina Lobanova and Emily Dinan and Eric Michael Smith and Filip Radenovic and Francisco Guzmán and Frank Zhang and Gabriel Synnaeve and Gabrielle Lee and Georgia Lewis Anderson and Govind Thattai and Graeme Nail and Gregoire Mialon and Guan Pang and Guillem Cucurell and Hailey Nguyen and Hannah Korevaar and Hu Xu and Hugo Touvron and Iliyan Zarov and Imanol Arrieta Ibarra and Isabel Kloumann and Ishan Misra and Ivan Evtimov and Jack Zhang and Jade Copet and Jaewon Lee and Jan Geffert and Jana Vranes and Jason Park and Jay Mahadeokar and Jeet Shah and Jelmer van der Linde and Jennifer Billock and Jenny Hong and Jenya Lee and Jeremy Fu and Jianfeng Chi and Jianyu Huang and Jiawen Liu and Jie Wang and Jiecao Yu and Joanna Bitton and Joe Spisak and Jongsoo Park and Joseph Rocca and Joshua Johnstun and Joshua Saxe and Junteng Jia and Kalyan Vasuden Alwala and Karthik Prasad and Kartikeya Upasani and Kate Plawiak and Ke Li and Kenneth Heafield and Kevin Stone and Khalid El-Arini and Krithika Iyer and Kshitiz Malik and Kuenley Chiu and Kunal Bhalla and Kushal Lakhotia and Lauren Rantala-Yeary and Laurens van der Maaten and Lawrence Chen and Liang Tan and Liz Jenkins and Louis Martin and Lovish Madaan and Lubo Malo and Lukas Blecher and Lukas Landzaat and Luke de Oliveira and Madeline Muzzi and Mahesh Pasupuleti and Mannat Singh and Manohar Paluri and Marcin Kardas and Maria Tsimpoukelli and Mathew Oldham and Mathieu Rita and Maya Pavlova and Melanie Kambadur and Mike Lewis and Min Si and Mitesh Kumar Singh and Mona Hassan and Naman Goyal and Narjes Torabi and Nikolay Bashlykov and Nikolay Bogoychev and Niladri Chatterji and Ning Zhang and Olivier Duchenne and Onur Çelebi and Patrick Alrassy and Pengchuan Zhang and Pengwei Li and Petar Vasic and Peter Weng and Prajjwal Bhargava and Pratik Dubal and Praveen Krishnan and Punit Singh Koura and Puxin Xu and Qing He and Qingxiao Dong and Ragavan Srinivasan and Raj Ganapathy and Ramon Calderer and Ricardo Silveira Cabral and Robert Stojnic and Roberta Raileanu and Rohan Maheswari and Rohit Girdhar and Rohit Patel and Romain Sauvestre and Ronnie Polidoro and Roshan Sumbaly and Ross Taylor and Ruan Silva and Rui Hou and Rui Wang and Saghar Hosseini and Sahana Chennabasappa and Sanjay Singh and Sean Bell and Seohyun Sonia Kim and Sergey Edunov and Shaoliang Nie and Sharan Narang and Sharath Raparthy and Sheng Shen and Shengye Wan and Shruti Bhosale and Shun Zhang and Simon Vandenhende and Soumya Batra and Spencer Whitman and Sten Sootla and Stephane Collot and Suchin Gururangan and Sydney Borodinsky and Tamar Herman and Tara Fowler and Tarek Sheasha and Thomas Georgiou and Thomas Scialom and Tobias Speckbacher and Todor Mihaylov and Tong Xiao and Ujjwal Karn and Vedanuj Goswami and Vibhor Gupta and Vignesh Ramanathan and Viktor Kerkez and Vincent Gonguet and Virginie Do and Vish Vogeti and Vítor Albiero and Vladan Petrovic and Weiwei Chu and Wenhan Xiong and Wenyin Fu and Whitney Meers and Xavier Martinet and Xiaodong Wang and Xiaofang Wang and Xiaoqing Ellen Tan and Xide Xia and Xinfeng Xie and Xuchao Jia and Xuewei Wang and Yaelle Goldschlag and Yashesh Gaur and Yasmine Babaei and Yi Wen and Yiwen Song and Yuchen Zhang and Yue Li and Yuning Mao and Zacharie Delpierre Coudert and Zheng Yan and Zhengxing Chen and Zoe Papakipos and Aaditya Singh and Aayushi Srivastava and Abha Jain and Adam Kelsey and Adam Shajnfeld and Adithya Gangidi and Adolfo Victoria and Ahuva Goldstand and Ajay Menon and Ajay Sharma and Alex Boesenberg and Alexei Baevski and Allie Feinstein and Amanda Kallet and Amit Sangani and Amos Teo and Anam Yunus and Andrei Lupu and Andres Alvarado and Andrew Caples and Andrew Gu and Andrew Ho and Andrew Poulton and Andrew Ryan and Ankit Ramchandani and Annie Dong and Annie Franco and Anuj Goyal and Aparajita Saraf and Arkabandhu Chowdhury and Ashley Gabriel and Ashwin Bharambe and Assaf Eisenman and Azadeh Yazdan and Beau James and Ben Maurer and Benjamin Leonhardi and Bernie Huang and Beth Loyd and Beto De Paola and Bhargavi Paranjape and Bing Liu and Bo Wu and Boyu Ni and Braden Hancock and Bram Wasti and Brandon Spence and Brani Stojkovic and Brian Gamido and Britt Montalvo and Carl Parker and Carly Burton and Catalina Mejia and Ce Liu and Changhan Wang and Changkyu Kim and Chao Zhou and Chester Hu and Ching-Hsiang Chu and Chris Cai and Chris Tindal and Christoph Feichtenhofer and Cynthia Gao and Damon Civin and Dana Beaty and Daniel Kreymer and Daniel Li and David Adkins and David Xu and Davide Testuggine and Delia David and Devi Parikh and Diana Liskovich and Didem Foss and Dingkang Wang and Duc Le and Dustin Holland and Edward Dowling and Eissa Jamil and Elaine Montgomery and Eleonora Presani and Emily Hahn and Emily Wood and Eric-Tuan Le and Erik Brinkman and Esteban Arcaute and Evan Dunbar and Evan Smothers and Fei Sun and Felix Kreuk and Feng Tian and Filippos Kokkinos and Firat Ozgenel and Francesco Caggioni and Frank Kanayet and Frank Seide and Gabriela Medina Florez and Gabriella Schwarz and Gada Badeer and Georgia Swee and Gil Halpern and Grant Herman and Grigory Sizov and Guangyi and Zhang and Guna Lakshminarayanan and Hakan Inan and Hamid Shojanazeri and Han Zou and Hannah Wang and Hanwen Zha and Haroun Habeeb and Harrison Rudolph and Helen Suk and Henry Aspegren and Hunter Goldman and Hongyuan Zhan and Ibrahim Damlaj and Igor Molybog and Igor Tufanov and Ilias Leontiadis and Irina-Elena Veliche and Itai Gat and Jake Weissman and James Geboski and James Kohli and Janice Lam and Japhet Asher and Jean-Baptiste Gaya and Jeff Marcus and Jeff Tang and Jennifer Chan and Jenny Zhen and Jeremy Reizenstein and Jeremy Teboul and Jessica Zhong and Jian Jin and Jingyi Yang and Joe Cummings and Jon Carvill and Jon Shepard and Jonathan McPhie and Jonathan Torres and Josh Ginsburg and Junjie Wang and Kai Wu and Kam Hou U and Karan Saxena and Kartikay Khandelwal and Katayoun Zand and Kathy Matosich and Kaushik Veeraraghavan and Kelly Michelena and Keqian Li and Kiran Jagadeesh and Kun Huang and Kunal Chawla and Kyle Huang and Lailin Chen and Lakshya Garg and Lavender A and Leandro Silva and Lee Bell and Lei Zhang and Liangpeng Guo and Licheng Yu and Liron Moshkovich and Luca Wehrstedt and Madian Khabsa and Manav Avalani and Manish Bhatt and Martynas Mankus and Matan Hasson and Matthew Lennie and Matthias Reso and Maxim Groshev and Maxim Naumov and Maya Lathi and Meghan Keneally and Miao Liu and Michael L. Seltzer and Michal Valko and Michelle Restrepo and Mihir Patel and Mik Vyatskov and Mikayel Samvelyan and Mike Clark and Mike Macey and Mike Wang and Miquel Jubert Hermoso and Mo Metanat and Mohammad Rastegari and Munish Bansal and Nandhini Santhanam and Natascha Parks and Natasha White and Navyata Bawa and Nayan Singhal and Nick Egebo and Nicolas Usunier and Nikhil Mehta and Nikolay Pavlovich Laptev and Ning Dong and Norman Cheng and Oleg Chernoguz and Olivia Hart and Omkar Salpekar and Ozlem Kalinli and Parkin Kent and Parth Parekh and Paul Saab and Pavan Balaji and Pedro Rittner and Philip Bontrager and Pierre Roux and Piotr Dollar and Polina Zvyagina and Prashant Ratanchandani and Pritish Yuvraj and Qian Liang and Rachad Alao and Rachel Rodriguez and Rafi Ayub and Raghotham Murthy and Raghu Nayani and Rahul Mitra and Rangaprabhu Parthasarathy and Raymond Li and Rebekkah Hogan and Robin Battey and Rocky Wang and Russ Howes and Ruty Rinott and Sachin Mehta and Sachin Siby and Sai Jayesh Bondu and Samyak Datta and Sara Chugh and Sara Hunt and Sargun Dhillon and Sasha Sidorov and Satadru Pan and Saurabh Mahajan and Saurabh Verma and Seiji Yamamoto and Sharadh Ramaswamy and Shaun Lindsay and Shaun Lindsay and Sheng Feng and Shenghao Lin and Shengxin Cindy Zha and Shishir Patil and Shiva Shankar and Shuqiang Zhang and Shuqiang Zhang and Sinong Wang and Sneha Agarwal and Soji Sajuyigbe and Soumith Chintala and Stephanie Max and Stephen Chen and Steve Kehoe and Steve Satterfield and Sudarshan Govindaprasad and Sumit Gupta and Summer Deng and Sungmin Cho and Sunny Virk and Suraj Subramanian and Sy Choudhury and Sydney Goldman and Tal Remez and Tamar Glaser and Tamara Best and Thilo Koehler and Thomas Robinson and Tianhe Li and Tianjun Zhang and Tim Matthews and Timothy Chou and Tzook Shaked and Varun Vontimitta and Victoria Ajayi and Victoria Montanez and Vijai Mohan and Vinay Satish Kumar and Vishal Mangla and Vlad Ionescu and Vlad Poenaru and Vlad Tiberiu Mihailescu and Vladimir Ivanov and Wei Li and Wenchen Wang and Wenwen Jiang and Wes Bouaziz and Will Constable and Xiaocheng Tang and Xiaojian Wu and Xiaolan Wang and Xilun Wu and Xinbo Gao and Yaniv Kleinman and Yanjun Chen and Ye Hu and Ye Jia and Ye Qi and Yenda Li and Yilin Zhang and Ying Zhang and Yossi Adi and Youngjin Nam and Yu and Wang and Yu Zhao and Yuchen Hao and Yundi Qian and Yunlu Li and Yuzi He and Zach Rait and Zachary DeVito and Zef Rosnbrick and Zhaoduo Wen and Zhenyu Yang and Zhiwei Zhao and Zhiyu Ma},
  year          = {2024},
  eprint        = {2407.21783},
  archiveprefix = {arXiv},
  primaryclass  = {cs.AI},
  url           = {https://arxiv.org/abs/2407.21783}
}

@misc{openai2024gpt4ocard,
  title         = {GPT-4o System Card},
  author        = {OpenAI},
  year          = {2024},
  eprint        = {2410.21276},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL},
  url           = {https://arxiv.org/abs/2410.21276}
}

@article{lewis2020retrieval,
  title   = {Retrieval-augmented generation for knowledge-intensive nlp tasks},
  author  = {Lewis, Patrick and Perez, Ethan and Piktus, Aleksandra and Petroni, Fabio and Karpukhin, Vladimir and Goyal, Naman and K{\"u}ttler, Heinrich and Lewis, Mike and Yih, Wen-tau and Rockt{\"a}schel, Tim and others},
  journal = {Advances in neural information processing systems},
  volume  = {33},
  pages   = {9459--9474},
  year    = {2020}
}

@misc{omni-benchmarking-ocr-2025,
  author       = {OmniAI},
  title        = {Benchmarking Open-Source Models for OCR},
  howpublished = {\url{https://getomni.ai/blog/benchmarking-open-source-models-for-ocr}},
  note         = {Omni AI Blog},
  year         = {2025},
  urldate      = {2025-04-24}
}

@article{hotpotqa2018,
  author     = {Zhilin Yang and
                Peng Qi and
                Saizheng Zhang and
                Yoshua Bengio and
                William W. Cohen and
                Ruslan Salakhutdinov and
                Christopher D. Manning},
  title      = {HotpotQA: {A} Dataset for Diverse, Explainable Multi-hop Question
                Answering},
  journal    = {CoRR},
  volume     = {abs/1809.09600},
  year       = {2018},
  url        = {http://arxiv.org/abs/1809.09600},
  eprinttype = {arXiv},
  eprint     = {1809.09600},
  timestamp  = {Fri, 05 Oct 2018 11:34:52 +0200},
  biburl     = {https://dblp.org/rec/journals/corr/abs-1809-09600.bib},
  bibsource  = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{cambazoglu2021review,
  title        = {A review of public datasets in question answering research},
  author       = {Cambazoglu, B Barla and Sanderson, Mark and Scholer, Falk and Croft, Bruce},
  booktitle    = {ACM SIGIR Forum},
  volume       = {54},
  number       = {2},
  pages        = {1--23},
  year         = {2021},
  organization = {ACM New York, NY, USA}
}

@misc{wang2023docllmlayoutawaregenerativelanguage,
  title         = {DocLLM: A layout-aware generative language model for multimodal document understanding},
  author        = {Dongsheng Wang and Natraj Raman and Mathieu Sibue and Zhiqiang Ma and Petr Babkin and Simerjot Kaur and Yulong Pei and Armineh Nourbakhsh and Xiaomo Liu},
  year          = {2023},
  eprint        = {2401.00908},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL},
  url           = {https://arxiv.org/abs/2401.00908}
}

@article{lewis2020retrieval,
  title   = {Retrieval-augmented generation for knowledge-intensive nlp tasks},
  author  = {Lewis, Patrick and Perez, Ethan and Piktus, Aleksandra and Petroni, Fabio and Karpukhin, Vladimir and Goyal, Naman and K{\"u}ttler, Heinrich and Lewis, Mike and Yih, Wen-tau and Rockt{\"a}schel, Tim and others},
  journal = {Advances in neural information processing systems},
  volume  = {33},
  pages   = {9459--9474},
  year    = {2020}
}

@misc{masry2022chartqabenchmarkquestionanswering,
  title         = {ChartQA: A Benchmark for Question Answering about Charts with Visual and Logical Reasoning},
  author        = {Ahmed Masry and Do Xuan Long and Jia Qing Tan and Shafiq Joty and Enamul Hoque},
  year          = {2022},
  eprint        = {2203.10244},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL},
  url           = {https://arxiv.org/abs/2203.10244}
}

@inproceedings{methani2020plotqa,
  title     = {Plotqa: Reasoning over scientific plots},
  author    = {Methani, Nitesh and Ganguly, Pritha and Khapra, Mitesh M and Kumar, Pratyush},
  booktitle = {Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision},
  pages     = {1527--1536},
  year      = {2020}
}

@misc{wcag22,
  author       = {{World Wide Web Consortium}},
  title        = {Web Content Accessibility Guidelines (WCAG) 2.2},
  howpublished = {W3C Recommendation},
  month        = oct,
  year         = {2024},
  url          = {https://www.w3.org/TR/WCAG22/},
  note         = {Accessed 2025-04-25}
}

@misc{hui2024udabenchmarksuiteretrieval,
  title         = {UDA: A Benchmark Suite for Retrieval Augmented Generation in Real-world Document Analysis},
  author        = {Yulong Hui and Yao Lu and Huanchen Zhang},
  year          = {2024},
  eprint        = {2406.15187},
  archiveprefix = {arXiv},
  primaryclass  = {cs.AI},
  url           = {https://arxiv.org/abs/2406.15187}
}

@inproceedings{smock2022pubtables,
  title     = {PubTables-1M: Towards comprehensive table extraction from unstructured documents},
  author    = {Smock, Brandon and Pesala, Rohith and Abraham, Robin},
  booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages     = {4634--4642},
  year      = {2022}
}

@misc{pypdf2024,
  author       = {{PyPDF Developers}},
  title        = {Py-pdf: A pure-python pdf library capable of splitting, merging, cropping, and transforming the pages of pdf files},
  year         = {2024},
  howpublished = {\url{https://github.com/py-pdf/pypdf}}
}

@misc{zheng2023judgingllmasajudgemtbenchchatbot,
  title         = {Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena},
  author        = {Lianmin Zheng and Wei-Lin Chiang and Ying Sheng and Siyuan Zhuang and Zhanghao Wu and Yonghao Zhuang and Zi Lin and Zhuohan Li and Dacheng Li and Eric P. Xing and Hao Zhang and Joseph E. Gonzalez and Ion Stoica},
  year          = {2023},
  eprint        = {2306.05685},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL},
  url           = {https://arxiv.org/abs/2306.05685}
}
@misc{PwC2024CaseStudy,
  author = {PricewaterhouseCoopers},
  title  = {Case Study: PwC entwickelt KI-Anwendung für Wissensmanagement beim ITZBund},
  year   = {2024},
  url    = {https://www.pwc.de/de/branchen-und-markte/oeffentlicher-sektor/case-study-ki-anwendung-fuer-wissensmanagement-beim-itzbund.html},
  note   = {Zugriff am 9. Mai 2025}
}
@misc{gao2024retrievalaugmentedgenerationlargelanguage,
  title         = {Retrieval-Augmented Generation for Large Language Models: A Survey},
  author        = {Yunfan Gao and Yun Xiong and Xinyu Gao and Kangxiang Jia and Jinliu Pan and Yuxi Bi and Yi Dai and Jiawei Sun and Meng Wang and Haofen Wang},
  year          = {2024},
  eprint        = {2312.10997},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL},
  url           = {https://arxiv.org/abs/2312.10997}
}
@misc{richards-2024,
  author = {Richards, David},
  month  = {3},
  title  = {{Retrieval Augmented Generation AI in Action: Real-World Case Studies Showcasing the Power of RAG – News from generation RAG}},
  year   = {2024},
  url    = {https://ragaboutit.com/retrieval-augmented-generation-ai-in-action-real-world-case-studies-showcasing-the-power-of-rag/}
}
@misc{cheng2025surveyknowledgeorientedretrievalaugmentedgeneration,
  title         = {A Survey on Knowledge-Oriented Retrieval-Augmented Generation},
  author        = {Mingyue Cheng and Yucong Luo and Jie Ouyang and Qi Liu and Huijie Liu and Li Li and Shuo Yu and Bohou Zhang and Jiawei Cao and Jie Ma and Daoyu Wang and Enhong Chen},
  year          = {2025},
  eprint        = {2503.10677},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL},
  url           = {https://arxiv.org/abs/2503.10677}
}

@article{arslan2024business,
  title     = {Business insights using RAG--LLMs: a review and case study},
  author    = {Arslan, Muhammad and Munawar, Saba and Cruz, Christophe},
  journal   = {Journal of Decision Systems},
  pages     = {1--30},
  year      = {2024},
  publisher = {Taylor \& Francis}
}
@misc{islam2023financebenchnewbenchmarkfinancial,
  title         = {FinanceBench: A New Benchmark for Financial Question Answering},
  author        = {Pranab Islam and Anand Kannappan and Douwe Kiela and Rebecca Qian and Nino Scherrer and Bertie Vidgen},
  year          = {2023},
  eprint        = {2311.11944},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL},
  url           = {https://arxiv.org/abs/2311.11944}
}

@article{Rogers_2023,
  title     = {QA Dataset Explosion: A Taxonomy of NLP Resources for Question Answering and Reading Comprehension},
  volume    = {55},
  issn      = {1557-7341},
  url       = {http://dx.doi.org/10.1145/3560260},
  doi       = {10.1145/3560260},
  number    = {10},
  journal   = {ACM Computing Surveys},
  publisher = {Association for Computing Machinery (ACM)},
  author    = {Rogers, Anna and Gardner, Matt and Augenstein, Isabelle},
  year      = {2023},
  month     = feb,
  pages     = {1–45}
}


@misc{wang2022modernquestionansweringdatasets,
  title         = {Modern Question Answering Datasets and Benchmarks: A Survey},
  author        = {Zhen Wang},
  year          = {2022},
  eprint        = {2206.15030},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL},
  url           = {https://arxiv.org/abs/2206.15030}
}

@inproceedings{chen-etal-2020-hybridqa,
  title     = {{H}ybrid{QA}: A Dataset of Multi-Hop Question Answering over Tabular and Textual Data},
  author    = {Chen, Wenhu  and
               Zha, Hanwen  and
               Chen, Zhiyu  and
               Xiong, Wenhan  and
               Wang, Hong  and
               Wang, William Yang},
  editor    = {Cohn, Trevor  and
               He, Yulan  and
               Liu, Yang},
  booktitle = {Findings of the Association for Computational Linguistics: EMNLP 2020},
  month     = nov,
  year      = {2020},
  address   = {Online},
  publisher = {Association for Computational Linguistics},
  url       = {https://aclanthology.org/2020.findings-emnlp.91/},
  doi       = {10.18653/v1/2020.findings-emnlp.91},
  pages     = {1026--1036},
  abstract  = {Existing question answering datasets focus on dealing with homogeneous information, based either only on text or KB/Table information alone. However, as human knowledge is distributed over heterogeneous forms, using homogeneous information alone might lead to severe coverage problems. To fill in the gap, we present HybridQA, a new large-scale question-answering dataset that requires reasoning on heterogeneous information. Each question is aligned with a Wikipedia table and multiple free-form corpora linked with the entities in the table. The questions are designed to aggregate both tabular information and text information, i.e., lack of either form would render the question unanswerable. We test with three different models: 1) a table-only model. 2) text-only model. 3) a hybrid model that combines heterogeneous information to find the answer. The experimental results show that the EM scores obtained by two baselines are below 20{\%}, while the hybrid model can achieve an EM over 40{\%}. This gap suggests the necessity to aggregate heterogeneous information in HybridQA. However, the hybrid model{'}s score is still far behind human performance. Hence, HybridQA can serve as a challenging benchmark to study question answering with heterogeneous information.}
}

@inproceedings{pfitzmann2022doclaynet,
  title     = {Doclaynet: A large human-annotated dataset for document-layout segmentation},
  author    = {Pfitzmann, Birgit and Auer, Christoph and Dolfi, Michele and Nassar, Ahmed S and Staar, Peter},
  booktitle = {Proceedings of the 28th ACM SIGKDD conference on knowledge discovery and data mining},
  pages     = {3743--3751},
  year      = {2022}
}

@inproceedings{chengM6DocLargeScaleMultiFormat2023,
  title      = {{{M6Doc}}: {{A Large-Scale Multi-Format}}, {{Multi-Type}}, {{Multi-Layout}}, {{Multi-Language}}, {{Multi-Annotation Category Dataset}} for {{Modern Document Layout Analysis}}},
  shorttitle = {{{M6Doc}}},
  booktitle  = {Proceedings of the {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author     = {Cheng, Hiuyi and Zhang, Peirong and Wu, Sihang and Zhang, Jiaxin and Zhu, Qiyuan and Xie, Zecheng and Li, Jing and Ding, Kai and Jin, Lianwen},
  year       = {2023},
  pages      = {15138--15147},
  urldate    = {2025-06-17},
  langid     = {english},
  file       = {/Users/paulsimmering/Zotero/storage/5E9V8ICY/Cheng et al. - 2023 - M6Doc A Large-Scale Multi-Format, Multi-Type, Multi-Layout, Multi-Language, Multi-Annotation Catego.pdf}
}

@inproceedings{ouyangOmniDocBenchBenchmarkingDiverse2025,
  title      = {{{OmniDocBench}}: {{Benchmarking Diverse PDF Document Parsing}} with {{Comprehensive Annotations}}},
  shorttitle = {{{OmniDocBench}}},
  author     = {Ouyang, Linke and Qu, Yuan and Zhou, Hongbin and Zhu, Jiawei and Zhang, Rui and Lin, Qunshu and Wang, Bin and Zhao, Zhiyuan and Jiang, Man and Zhao, Xiaomeng and Shi, Jin and Wu, Fan and Chu, Pei and Liu, Minghao and Li, Zhenxiang and Xu, Chao and Zhang, Bo and Shi, Botian and Tu, Zhongying and He, Conghui},
  date       = {2025},
  pages      = {24838--24848},
  url        = {https://openaccess.thecvf.com/content/CVPR2025/html/Ouyang_OmniDocBench_Benchmarking_Diverse_PDF_Document_Parsing_with_Comprehensive_Annotations_CVPR_2025_paper.html},
  urldate    = {2025-06-17},
  eventtitle = {Proceedings of the {{Computer Vision}} and {{Pattern Recognition Conference}}},
  langid     = {english},
  file       = {/Users/paulsimmering/Zotero/storage/UPSERZGU/Ouyang et al. - 2025 - OmniDocBench Benchmarking Diverse PDF Document Parsing with Comprehensive Annotations.pdf}
}

@online{luoLayoutLLMLayoutInstruction2024,
  title       = {{{LayoutLLM}}: {{Layout Instruction Tuning}} with {{Large Language Models}} for {{Document Understanding}}},
  shorttitle  = {{{LayoutLLM}}},
  author      = {Luo, Chuwei and Shen, Yufan and Zhu, Zhaoqing and Zheng, Qi and Yu, Zhi and Yao, Cong},
  date        = {2024-04-08},
  eprint      = {2404.05225},
  eprinttype  = {arXiv},
  eprintclass = {cs},
  doi         = {10.48550/arXiv.2404.05225},
  url         = {http://arxiv.org/abs/2404.05225},
  urldate     = {2025-06-17},
  abstract    = {Recently, leveraging large language models (LLMs) or multimodal large language models (MLLMs) for document understanding has been proven very promising. However, previous works that employ LLMs/MLLMs for document understanding have not fully explored and utilized the document layout information, which is vital for precise document understanding. In this paper, we propose LayoutLLM, an LLM/MLLM based method for document understanding. The core of LayoutLLM is a layout instruction tuning strategy, which is specially designed to enhance the comprehension and utilization of document layouts. The proposed layout instruction tuning strategy consists of two components: Layout-aware Pre-training and Layout-aware Supervised Fine-tuning. To capture the characteristics of document layout in Layout-aware Pre-training, three groups of pre-training tasks, corresponding to document-level, region-level and segment-level information, are introduced. Furthermore, a novel module called layout chain-of-thought (LayoutCoT) is devised to enable LayoutLLM to focus on regions relevant to the question and generate accurate answers. LayoutCoT is effective for boosting the performance of document understanding. Meanwhile, it brings a certain degree of interpretability, which could facilitate manual inspection and correction. Experiments on standard benchmarks show that the proposed LayoutLLM significantly outperforms existing methods that adopt open-source 7B LLMs/MLLMs for document understanding. The training data of the LayoutLLM is publicly available at https://github.com/AlibabaResearch/AdvancedLiterateMachinery/tree/main/DocumentUnderstanding/LayoutLLM},
  pubstate    = {prepublished},
  keywords    = {Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition},
  file        = {/Users/paulsimmering/Zotero/storage/WUAJYGLB/Luo et al. - 2024 - LayoutLLM Layout Instruction Tuning with Large Language Models for Document Understanding.pdf;/Users/paulsimmering/Zotero/storage/RYKCV5YE/2404.html}
}

@online{zerox_github,
  title   = {zerox},
  author  = {{OmniAI}},
  year    = {2024},
  url     = {https://github.com/getomni-ai/zerox},
  urldate = {2025-06-17},
  note    = {GitHub repository}
}

@online{marker_github,
  title   = {marker},
  author  = {{Datalab}},
  year    = {2024},
  url     = {https://github.com/datalab-to/marker},
  urldate = {2025-06-25},
  note    = {GitHub repository}
}

@misc{chenMDEvalEvaluatingEnhancing2025,
  title         = {{{MDEval}}: {{Evaluating}} and {{Enhancing Markdown Awareness}} in {{Large Language Models}}},
  shorttitle    = {{{MDEval}}},
  author        = {Chen, Zhongpu and Liu, Yinfeng and Shi, Long and Wang, Zhi-Jie and Chen, Xingyan and Zhao, Yu and Ren, Fuji},
  year          = {2025},
  month         = jan,
  number        = {arXiv:2501.15000},
  eprint        = {2501.15000},
  primaryclass  = {cs},
  publisher     = {arXiv},
  doi           = {10.48550/arXiv.2501.15000},
  urldate       = {2025-07-04},
  abstract      = {Large language models (LLMs) are expected to offer structured Markdown responses for the sake of readability in web chatbots (e.g., ChatGPT). Although there are a myriad of metrics to evaluate LLMs, they fail to evaluate the readability from the view of output content structure. To this end, we focus on an overlooked yet important metric -- Markdown Awareness, which directly impacts the readability and structure of the content generated by these language models. In this paper, we introduce MDEval, a comprehensive benchmark to assess Markdown Awareness for LLMs, by constructing a dataset with 20K instances covering 10 subjects in English and Chinese. Unlike traditional model-based evaluations, MDEval provides excellent interpretability by combining model-based generation tasks and statistical methods. Our results demonstrate that MDEval achieves a Spearman correlation of 0.791 and an accuracy of 84.1\% with human, outperforming existing methods by a large margin. Extensive experimental results also show that through fine-tuning over our proposed dataset, less performant open-source models are able to achieve comparable performance to GPT-4o in terms of Markdown Awareness. To ensure reproducibility and transparency, MDEval is open sourced at https://github.com/SWUFE-DB-Group/MDEval-Benchmark.},
  archiveprefix = {arXiv},
  keywords      = {Computer Science - Computation and Language,Computer Science - Information Retrieval},
  file          = {/Users/paulsimmering/Zotero/storage/6SX2YTI7/Chen et al. - 2025 - MDEval Evaluating and Enhancing Markdown Awareness in Large Language Models.pdf;/Users/paulsimmering/Zotero/storage/RJU6YJGZ/2501.html}
}

@online{llama4_meta_blog,
  title  = {Llama 4: Advancing Multimodal Intelligence},
  author = {{Meta AI}},
  year   = {2025},
  url    = {https://ai.meta.com/blog/llama-4-multimodal-intelligence/},
  note   = {Meta AI Blog}
}

@misc{qwenQwen25TechnicalReport2025,
  title         = {Qwen2.5 {{Technical Report}}},
  author        = {Yang, An and Yang, Baosong and Zhang, Beichen and Hui, Binyuan and Zheng, Bo and Yu, Bowen and Li, Chengyuan and Liu, Dayiheng and Huang, Fei and Wei, Haoran and Lin, Huan and Yang, Jian and Tu, Jianhong and Zhang, Jianwei and Yang, Jianxin and Yang, Jiaxi and Zhou, Jingren and Lin, Junyang and Dang, Kai and Lu, Keming and Bao, Keqin and Yang, Kexin and Yu, Le and Li, Mei and Xue, Mingfeng and Zhang, Pei and Zhu, Qin and Men, Rui and Lin, Runji and Li, Tianhao and Tang, Tianyi and Xia, Tingyu and Ren, Xingzhang and Ren, Xuancheng and Fan, Yang and Su, Yang and Zhang, Yichang and Wan, Yu and Liu, Yuqiong and Cui, Zeyu and Zhang, Zhenru and Qiu, Zihan},
  year          = {2025},
  month         = jan,
  number        = {arXiv:2412.15115},
  eprint        = {2412.15115},
  primaryclass  = {cs},
  publisher     = {arXiv},
  doi           = {10.48550/arXiv.2412.15115},
  urldate       = {2025-07-10},
  abstract      = {In this report, we introduce Qwen2.5, a comprehensive series of large language models (LLMs) designed to meet diverse needs. Compared to previous iterations, Qwen 2.5 has been significantly improved during both the pre-training and post-training stages. In terms of pre-training, we have scaled the high-quality pre-training datasets from the previous 7 trillion tokens to 18 trillion tokens. This provides a strong foundation for common sense, expert knowledge, and reasoning capabilities. In terms of post-training, we implement intricate supervised finetuning with over 1 million samples, as well as multistage reinforcement learning. Post-training techniques enhance human preference, and notably improve long text generation, structural data analysis, and instruction following. To handle diverse and varied use cases effectively, we present Qwen2.5 LLM series in rich sizes. Open-weight offerings include base and instruction-tuned models, with quantized versions available. In addition, for hosted solutions, the proprietary models currently include two mixture-of-experts (MoE) variants: Qwen2.5-Turbo and Qwen2.5-Plus, both available from Alibaba Cloud Model Studio. Qwen2.5 has demonstrated top-tier performance on a wide range of benchmarks evaluating language understanding, reasoning, mathematics, coding, human preference alignment, etc. Specifically, the open-weight flagship Qwen2.5-72B-Instruct outperforms a number of open and proprietary models and demonstrates competitive performance to the state-of-the-art open-weight model, Llama-3-405B-Instruct, which is around 5 times larger. Qwen2.5-Turbo and Qwen2.5-Plus offer superior cost-effectiveness while performing competitively against GPT-4o-mini and GPT-4o respectively. Additionally, as the foundation, Qwen2.5 models have been instrumental in training specialized models such as Qwen2.5-Math, Qwen2.5-Coder, QwQ, and multimodal models.},
  archiveprefix = {arXiv},
  keywords      = {Computer Science - Computation and Language},
  file          = {/Users/paulsimmering/Zotero/storage/72A2YYZK/Qwen et al. - 2025 - Qwen2.5 Technical Report.pdf;/Users/paulsimmering/Zotero/storage/EF53DZAE/2412.html}
}
